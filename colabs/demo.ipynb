{
  "cells": [
    {
      "metadata": {
        "id": "aPF2PXURwiWb"
      },
      "cell_type": "markdown",
      "source": [
        "#### Copyright 2025 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "   http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "metadata": {
        "id": "G5902VHAnDD_"
      },
      "cell_type": "markdown",
      "source": [
        "# SciVid: Cross-Domain Evaluation of Video Models in Scientific Applications\n",
        "\n",
        "*Yana Hasson, Pauline Luc, Liliane Momeni, Maks Ovsjanikov, Guillaume Le Moing, Alina Kuznetsova, Ira Ktena, Jennifer J. Sun, Skanda Koppula, Dilara Gokay, Joseph Heyward, Etienne Pot, Andrew Zisserman*\n",
        "\n",
        "[Paper](https://arxiv.org/abs/2507.03578) | [GitHub](https://github.com/google-deepmind/scivid)\n",
        "\n",
        "\n",
        "# Colab demo\n",
        "\n",
        "This Colab provides a hands-on demonstration to:\n",
        "\n",
        "- Visualize data samples from the five scientific video datasets included in SciVid.\n",
        "\n",
        "![image](https://storage.googleapis.com/scivid/assets/scivid_overview.gif)\n",
        "\n",
        "- Initialize and visualize a model composed of a video backbone and a task-specific readout.\n",
        "\n",
        "- Run inference and visualize predictions.\n",
        "\n",
        "\u003cimg src=\"https://storage.googleapis.com/scivid/assets/evaluation_overview.png\" alt=\"Evaluation overview\" width=\"55%\"\u003e\n",
        "\n",
        "**Evaluation overview**. For each task, we train a lightweight readout on top of the backbone.\n"
      ]
    },
    {
      "metadata": {
        "id": "LvO12DWEbEx2"
      },
      "cell_type": "markdown",
      "source": [
        "# Setup SciVid code and data"
      ]
    },
    {
      "metadata": {
        "cellView": "form",
        "id": "TQWD1QmyXyKh"
      },
      "cell_type": "code",
      "source": [
        "# @title Download SciVid code from github\n",
        "\n",
        "# Set directory to which the scivid code will be downloaded\n",
        "SCIVID_CODE_DIR = '/content/scivid'  # @param {type:\"string\", isTemplate: true}\n",
        "\n",
        "# Download scivid code from github to target directory\n",
        "!git clone https://github.com/google-deepmind/scivid {SCIVID_CODE_DIR}\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "cellView": "form",
        "id": "Ih4A6CRXYHDO"
      },
      "cell_type": "code",
      "source": [
        "# @title Install scivid with dependencies\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Install scivid repository in editable mode,\n",
        "!pip install -e {SCIVID_CODE_DIR}\n",
        "\n",
        "# Add the parent directory of the scivid package to sys.path.\n",
        "scivid_parent_dir = os.path.dirname(SCIVID_CODE_DIR)\n",
        "if scivid_parent_dir not in sys.path:\n",
        "    sys.path.append(scivid_parent_dir)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "cellView": "form",
        "id": "P2T3mE8E4AMM"
      },
      "cell_type": "code",
      "source": [
        "# @title Mount the SciVid data bucket\n",
        "SCIVID_DATA_DIR = '/content/data/scivid'  # @param {type:\"string\", isTemplate: true}\n",
        "\n",
        "# Create local folder where SciVid data will be mounted\n",
        "!mkdir -p {SCIVID_DATA_DIR}\n",
        "\n",
        "# Install gcsfuse using fix from stackoverflow.com/q/68568808\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "!echo \"deb https://packages.cloud.google.com/apt gcsfuse-bionic main\" | sudo tee /etc/apt/sources.list.d/gcsfuse.list\n",
        "!apt-get update\n",
        "!apt-get install gcsfuse\n",
        "\n",
        "# Authenticate with Google Cloud (if not already done)\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Mount the bucket\n",
        "!gcsfuse --implicit-dirs scivid {SCIVID_DATA_DIR}\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "cellView": "form",
        "id": "M_JSIkXAYnwt"
      },
      "cell_type": "code",
      "source": [
        "# @title Set SCIVID_DATA_DIR environment variable to data location\n",
        "\n",
        "# This variable is used by the data readers to locate the data.\n",
        "%env SCIVID_DATA_DIR={SCIVID_DATA_DIR}"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "DB9b9lSrZs8x"
      },
      "cell_type": "markdown",
      "source": [
        "# Initialize [kauldron](https://github.com/google-research/kauldron) trainer and display model and example batch\n"
      ]
    },
    {
      "metadata": {
        "cellView": "form",
        "id": "7pdeRxYJYppG"
      },
      "cell_type": "code",
      "source": [
        "# @title Initialize kauldron trainer.\n",
        "\n",
        "from etils import ecolab\n",
        "from kauldron import kd\n",
        "from scivid.configs import launch_config\n",
        "\n",
        "\n",
        "MODEL_NAME = \"mock_model\"  # @param [\"mock_model\", \"hf_videomae\", \"scaling4d\"] {allow-input: true}\n",
        "EVAL_NAME = \"calms21_classification\"  # @param [\"flyvsfly_classification\", \"calms21_classification\", \"typhoon_future_pred\", \"weatherbench_future_pred\", \"stir_2d_tracking\"] {allow-input: true}\n",
        "\n",
        "# If using scaling4d model, download pretrained checkpoint.\n",
        "# We load the weights from the 4DS-B-dist-e released checkpoint which is a\n",
        "# B model distilled from 4DS-e. We refer readers to Table 2 of the\n",
        "# [Scaling 4D Representation](https://arxiv.org/pdf/2412.15212)\n",
        "# paper for a comparison between different 4DS model variants.\n",
        "SCALING4D_CHECKPOINT_LOCAL_DIR = \"/content/models\"  # @param {type:\"string\"}\n",
        "SCALING4D_CHECKPOINT_NAME = \"scaling4d_dist_b.npz\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set appropriate environment variable\n",
        "%env SCALING4D_CHECKPOINT_PATH={SCALING4D_CHECKPOINT_LOCAL_DIR}/{SCALING4D_CHECKPOINT_NAME}\n",
        "\n",
        "if MODEL_NAME == \"scaling4d\":\n",
        "  !mkdir -p {SCALING4D_CHECKPOINT_LOCAL_DIR}\n",
        "  # Construct the checkpoint URL\n",
        "  checkpoint_url = os.path.join(\n",
        "      \"https://storage.googleapis.com/representations4d/checkpoints\",\n",
        "      SCALING4D_CHECKPOINT_NAME,\n",
        "  )\n",
        "  # Download checkpoint to local directory\n",
        "  print(\n",
        "      f\"Downloading {checkpoint_url} to\"\n",
        "      f\" {SCALING4D_CHECKPOINT_LOCAL_DIR}\"\n",
        "  )\n",
        "  !wget -O {SCALING4D_CHECKPOINT_LOCAL_DIR}/{SCALING4D_CHECKPOINT_NAME} {checkpoint_url}\n",
        "\n",
        "# Where the experiment artefacts are stored\n",
        "WORKDIR = \"/content/tmp/workdir\"  # @param {type:\"string\"}\n",
        "\n",
        "# Initialize the kd.train.Trainer configwhich defines which model, dataset and\n",
        "# metrics to use\n",
        "cfg = launch_config.get_config(f\"{MODEL_NAME}:{EVAL_NAME}\")\n",
        "cfg.workdir = WORKDIR\n",
        "\n",
        "with ecolab.collapse('Config (modified)'):\n",
        "  cfg;\n",
        "\n",
        "trainer = kd.konfig.resolve(cfg)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "cellView": "form",
        "id": "kLK1Qf4UYvHw"
      },
      "cell_type": "code",
      "source": [
        "# @title Get and visualize one batch from the training dataset\n",
        "\n",
        "# Get first batch from the training dataset\n",
        "batch = next(iter(trainer.train_ds))\n",
        "\n",
        "with ecolab.collapse(\"Batch statistics\"):\n",
        "  ecolab.disp(kd.inspect.get_batch_stats(batch))\n",
        "\n",
        "with ecolab.collapse(\"Batch images\"):\n",
        "  kd.inspect.plot_batch(batch)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "cellView": "form",
        "id": "IPdEtAYbYxBH"
      },
      "cell_type": "code",
      "source": [
        "# @title Visualize model architecture\n",
        "model = trainer.model\n",
        "\n",
        "model_overview = kd.inspect.get_colab_model_overview(\n",
        "    model=model,\n",
        "    # model_config=None if cfg is None else cfg.model,\n",
        "    train_ds=trainer.train_ds,\n",
        "    ds_sharding=trainer.sharding.ds,\n",
        "    rngs=trainer.rng_streams.init_rngs(),\n",
        ")\n",
        "\n",
        "# Compute total number of parameters\n",
        "total_params = model_overview['Own Params'].sum()\n",
        "\n",
        "# Display model structure in collapsible window.\n",
        "with ecolab.collapse(f'Model Overview (#Params: {total_params:,})'):\n",
        "  model_overview;\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "cellView": "form",
        "id": "P0XqUEEYAUZ9"
      },
      "cell_type": "code",
      "source": [
        "# @title Run forward pass\n",
        "\n",
        "# Add metrics, losses and summaries\n",
        "aux_wrapper = kd.train.Auxiliaries(\n",
        "    losses=trainer.train_losses,\n",
        "    metrics=trainer.train_metrics,\n",
        "    summaries=trainer.train_summaries,\n",
        ")\n",
        "\n",
        "# Initialize model state\n",
        "trainstep = kd.train.TrainStep(\n",
        "    model=trainer.model,\n",
        "    optimizer=trainer.optimizer,\n",
        "    rng_streams=trainer.rng_streams,\n",
        "    sharding=trainer.sharding,\n",
        "    init_transform=trainer.init_transform,\n",
        "    aux=aux_wrapper,\n",
        ")\n",
        "element_spec = trainer.train_ds.element_spec\n",
        "state = trainstep.init(element_spec)\n",
        "\n",
        "\n",
        "# Run the model\n",
        "context = kd.train.Context.from_state_and_batch(state=state, batch=batch)\n",
        "context = kd.train.forward(\n",
        "    context,\n",
        "    model=trainer.model,\n",
        "    rngs=trainstep.rng_streams.train_rngs(state.step),\n",
        "    is_training=True,\n",
        ")\n",
        "\n",
        "# Display prediction summary\n",
        "predictions = context.preds\n",
        "with ecolab.collapse('Predictions'):\n",
        "  ecolab.disp(kd.inspect.get_batch_stats(predictions))"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//gdm/vision/scenes4d:gdm_4d_scenes_ml_notebook",
        "kind": "private"
      },
      "name": "scivid_demo.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
